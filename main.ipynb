{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATisiT\\Anaconda3\\lib\\site-packages\\deap\\tools\\_hypervolume\\pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from deap import creator, base, tools, algorithms\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(l):\n",
    "    \"\"\"\n",
    "    Returns the average between list elements\n",
    "    \"\"\"\n",
    "    return (sum(l)/float(len(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFitness(individual, X, y): ## ds\n",
    "    \"\"\"\n",
    "    Feature subset fitness function\n",
    "    \"\"\"\n",
    "\n",
    "    if(individual.count(0) != len(individual)):\n",
    "        # get index with value 0\n",
    "        cols = [index for index in range(\n",
    "            len(individual)) if individual[index] == 0]\n",
    "\n",
    "        # get features subset\n",
    "        X_parsed = X.drop(X.columns[cols], axis=1)\n",
    "        X_subset = pd.get_dummies(X_parsed)\n",
    "\n",
    "        # apply classification algorithm\n",
    "        clf = LogisticRegression()\n",
    "\n",
    "        return (avg(cross_val_score(clf, X_subset, y, cv=5)),)\n",
    "    else:\n",
    "        return(0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm(X, y, n_population, n_generation):\n",
    "    \"\"\"\n",
    "    Deap global variables\n",
    "    Initialize variables to use eaSimple\n",
    "    \"\"\"\n",
    "    # create individual\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "    # create toolbox\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "    toolbox.register(\"individual\", tools.initRepeat,\n",
    "                     creator.Individual, toolbox.attr_bool, len(X.columns))\n",
    "    toolbox.register(\"population\", tools.initRepeat, list,\n",
    "                     toolbox.individual)\n",
    "    toolbox.register(\"evaluate\", getFitness, X=X, y=y)\n",
    "    toolbox.register(\"mate\", tools.cxOnePoint)\n",
    "    toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "    # initialize parameters\n",
    "    pop = toolbox.population(n=n_population)\n",
    "\n",
    "    hof = tools.HallOfFame(n_population * n_generation)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "    \n",
    "    print(\"xx\")\n",
    "    # genetic algorithm\n",
    "    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2,\n",
    "                                   ngen=n_generation, stats=stats, halloffame=hof,\n",
    "                                   verbose=True)\n",
    "    print(\" hall of frame :\", hof.maxsize)\n",
    "\n",
    "    # return hall of fame\n",
    "    return hof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestIndividual(hof, X, y):\n",
    "    \"\"\"\n",
    "    Get the best individual\n",
    "    \"\"\"\n",
    "    maxAccurcy = 0.0\n",
    "    for individual in hof:\n",
    "        print(type(individual.fitness.values[0]))\n",
    "        if(individual.fitness.values[0] > maxAccurcy):\n",
    "            maxAccurcy = individual.fitness.values[0]\n",
    "            _individual = individual\n",
    "\n",
    "    _individualHeader = [list(X)[i] for i in range(\n",
    "        len(_individual)) if _individual[i] == 1]\n",
    "    return _individual.fitness.values, _individual, _individualHeader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArguments():\n",
    "    \"\"\"\n",
    "    Get argumments from command-line\n",
    "    If pass only dataframe path, pop and gen will be default\n",
    "    \"\"\"\n",
    "    dfPath = sys.argv[1]\n",
    "    if(len(sys.argv) == 4):\n",
    "        pop = int(sys.argv[2])\n",
    "        gen = int(sys.argv[3])\n",
    "    else:\n",
    "        pop = 10\n",
    "        gen = 2\n",
    "    return dfPath, pop, gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # get dataframe path, population number and generation number from command-line argument\n",
    "# n_pop = 20\n",
    "# n_gen = 6\n",
    "# # read dataframe from csv\n",
    "# df = pd.read_csv('datasets/nuclear.csv', sep=',')\n",
    "\n",
    "# # encode labels column to numbers\n",
    "# le = LabelEncoder()\n",
    "# le.fit(df.iloc[:, -1])\n",
    "# y = le.transform(df.iloc[:, -1]) # label\n",
    "# y_test = y[:20]\n",
    "# X = df.iloc[:, :-1] # data\n",
    "\n",
    "# # get accuracy with all features\n",
    "# individual = [1 for i in range(len(X.columns))] # true column (feature)\n",
    "# print(\"Accuracy with all features: \\t\" +\n",
    "#       str(getFitness(individual, X, y)) + \"\\n\")\n",
    "\n",
    "# # apply genetic algorithm\n",
    "# hof = geneticAlgorithm(X, y, n_pop, n_gen)\n",
    "\n",
    "# # select the best individual\n",
    "# accuracy, individual, header = bestIndividual(hof, X, y)\n",
    "# print('Best Accuracy: \\t' + str(accuracy))\n",
    "# print('Number of Features in Subset: \\t' + str(individual.count(1)))\n",
    "# print('Individual: \\t\\t' + str(individual))\n",
    "# print('Feature Subset\\t: ' + str(header))\n",
    "\n",
    "# print('\\n\\ncreating a new classifier with the result')\n",
    "\n",
    "# # read dataframe from csv one more time\n",
    "# df = pd.read_csv('datasets/nuclear.csv', sep=',')\n",
    "\n",
    "# # with feature subset\n",
    "# X = df[header]\n",
    "\n",
    "# clf = LogisticRegression()\n",
    "\n",
    "# scores = cross_val_score(clf, X, y, cv=5)\n",
    "# print(\"Accuracy with Feature Subset: \\t\" + str(avg(scores)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "Accuracy with all features: \t(0.94422514619883047,)\n",
      "\n",
      "xx\n",
      "gen\tnevals\tavg     \tmin\tmax     \n",
      "0  \t100   \t0.779441\t0  \t0.944225\n",
      "1  \t53    \t0.90346 \t0.700658\t0.944225\n",
      "2  \t65    \t0.932829\t0.767544\t0.944225\n",
      "3  \t67    \t0.94389 \t0.910673\t0.944225\n",
      "4  \t64    \t0.942107\t0.900146\t0.944225\n",
      "5  \t57    \t0.943554\t0.910673\t0.944225\n",
      "6  \t43    \t0.943554\t0.910673\t0.944225\n",
      "7  \t58    \t0.943554\t0.910673\t0.944225\n",
      "8  \t63    \t0.93981 \t0.700658\t0.944225\n",
      "9  \t62    \t0.941017\t0.822734\t0.944225\n",
      "10 \t54    \t0.943554\t0.910673\t0.944225\n",
      "11 \t62    \t0.942126\t0.900146\t0.944225\n",
      "12 \t58    \t0.943113\t0.900146\t0.944225\n",
      "13 \t69    \t0.942673\t0.900146\t0.944225\n",
      "14 \t65    \t0.944225\t0.944225\t0.944225\n",
      "15 \t66    \t0.942462\t0.900146\t0.944225\n",
      "16 \t61    \t0.943554\t0.910673\t0.944225\n",
      "17 \t60    \t0.943219\t0.910673\t0.944225\n",
      "18 \t59    \t0.943219\t0.910673\t0.944225\n",
      "19 \t56    \t0.943784\t0.900146\t0.944225\n",
      "20 \t55    \t0.94389 \t0.910673\t0.944225\n",
      " hall of frame : 2000\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'float'>\n",
      "Best Accuracy: \t(0.94422514619883047,)\n",
      "Number of Features in Subset: \t4\n",
      "Individual: \t\t[1, 1, 1, 1]\n",
      "Feature Subset\t: ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\n",
      "\n",
      "\n",
      "creating a new classifier with the result\n",
      "Accuracy with Feature Subset: \t0.944225146199\n",
      "\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': 3, 'max_features': 3, 'min_samples_leaf': 3}\n",
      "Best score is 0.9666666666666667\n",
      "Test acc : 0.95\n"
     ]
    }
   ],
   "source": [
    "n_pop = 100\n",
    "n_gen = 20\n",
    "# read dataframe from csv\n",
    "df = pd.read_csv('datasets/iris.csv',)\n",
    "le = LabelEncoder()\n",
    "le.fit(df.iloc[:, -1])\n",
    "y = le.transform(df.iloc[:, -1]) # label\n",
    "print(le.classes_)\n",
    "print(y)\n",
    "X = df.drop([df.columns[0], 'Species'], axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train, y_test = train_test_split(X , y , test_size=0.4 , random_state=0)\n",
    "\n",
    "# get accuracy with all features\n",
    "individual = [1 for i in range(len(x_train.columns))] # true column (feature)\n",
    "print(\"Accuracy with all features: \\t\" +\n",
    "      str(getFitness(individual, x_train, y_train)) + \"\\n\")\n",
    "\n",
    "\n",
    "# apply genetic algorithm\n",
    "hof = geneticAlgorithm(x_train, y_train , n_pop, n_gen)\n",
    "\n",
    "# select the best individual\n",
    "accuracy, individual, header = bestIndividual(hof, x_train, y_train)\n",
    "print('Best Accuracy: \\t' + str(accuracy))\n",
    "print('Number of Features in Subset: \\t' + str(individual.count(1)))\n",
    "print('Individual: \\t\\t' + str(individual))\n",
    "print('Feature Subset\\t: ' + str(header))\n",
    "\n",
    "print('\\n\\ncreating a new classifier with the result')\n",
    "\n",
    "# read dataframe from csv one more time\n",
    "# df = pd.read_csv('datasets/iris.csv', sep=',')\n",
    "\n",
    "# with feature subset\n",
    "x_train = x_train[header]\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
    "print(\"Accuracy with Feature Subset: \\t\" + str(avg(scores)) + \"\\n\")\n",
    "\n",
    "# Import necessary modules\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, len(header)),\n",
    "              \"min_samples_leaf\": randint(1, len(header)),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object: tree_cv\n",
    "tree_cv = RandomizedSearchCV(DecisionTreeClassifier(), param_dist, cv=5)\n",
    "tree_cv2 = RandomizedSearchCV(DecisionTreeClassifier(), param_dist, cv=5)\n",
    "# Fit it to the data\n",
    "tree_cv.fit(x_train, y_train)\n",
    "predicted = tree_cv.predict(x_test)\n",
    "# Print the tuned parameters and score\n",
    "\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))\n",
    "\n",
    "\n",
    "# model = LogisticRegression()\n",
    "# model.fit(x_train, y_train)\n",
    "# predicted = model.predict(x_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "scores = accuracy_score(predicted, y_test)\n",
    "print(\"Test acc : {}\".format(scores))\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "score = cross_val_score(tree_cv2, x_train, y_train, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
